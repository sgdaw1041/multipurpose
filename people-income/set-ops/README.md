**Setup schema with tables, create 3 sampled datafiles, and load tables** 

1. create 3 sampled datafiles from original dataset. Within local system, use either python
    
```
    ~./people-income/set-ops$ python3  py_create_samples.py

```

   or use sql. Note, files must be created by docker container root.

```
    people_income=> \q
    /home/people-income $ exit
    /home/people-income # exit
    sudo docker container exec -it psql14_compose psql people_income -U user-name -f ./home/people-income/set-ops/db_create_samples.sql

```

2. create schema and all tables
``` 
    /home/people-income $ cd set-ops
    /home/people-income/set-ops $ psql people_income -U user-name -f  create_set_ops.sql
    /home/people-income/set-ops $ psql people_income -U user-name
    people_income=> \dn
    
```


3. import data from 3 csv-files--generated by either python or sql--to respective 3 tables 
```
    people_income=> set search_path to  set_query, data_query;
    people_income=> \d+
    people_income=> \i  db_populate_samples.sql
``` 

4. run set-ops script to load remaining tables and perform analysis

``` 
    people_income=> \i db_set_ops.sql
    
```

**Contents** 

- ```create_set_ops.sql``` create schema and tables.
- ```db_create_samples.sql``` create 3 csv-file samples with no duplicate rows per sample from main table.
- ```py_create_samples.py``` create 3 csv-file samples with no replacement from main dataset.
- ```db_populate_samples.sql``` load 3 csv-files sampled from main table; load either python or db generated files.
- ```db_set_ops.sql``` set operation database queries.
- ```py_set_ops.py ``` set operation python code.


